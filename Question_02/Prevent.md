

## AI-Assisted Code: Issues & Prevention

### How AI Assistance Could Have Contributed

#### Issue 1: Overly Optimistic Timeouts in E2E Tests (Bug #5)

**What happened:**

- AI generated E2E test with hardcoded `timeout=5000` (5 seconds)
- Example code copied from documentation (ideal scenario, localhost)
- Real staging proxy requests take 8-12 seconds (network latency, queue time)
- Tests fail intermittently when staging is under load

**AI-generated code (problematic):**

```python
# Generated by AI - copied from example docs
def test_proxy_request_via_api_key():
    response = requests.get(
        "https://staging-proxy.company.com/request",
        headers={"Authorization": f"Bearer {api_key}"},
        timeout=5  # ❌ Too optimistic for real network
    )
    assert response.status_code == 200
```

**Why AI caused this:**

- AI trains on documentation/examples (optimized for readability, not production)
- Doesn't understand real-world network conditions (latency, retries, load)
- Assumes "happy path" (no queue, instant response)

**How to detect:**

- ⚠️ Test flakiness metrics spike (>5% failure rate on specific tests)
- ⚠️ Pattern: Failures only in CI (staging), never local
- ⚠️ Error logs show `requests.exceptions.Timeout`

**How to prevent:**

**1. Code review checklist:**

```
- [ ] Timeouts are environment-aware (dev: 5s, staging: 30s, prod: 60s)
- [ ] Network calls use retry logic (exponential backoff)
- [ ] Tests don't assume instant response
```

**2. Lint rules (custom):**

```python
# pre-commit hook
def check_hardcoded_timeouts(code):
    if re.search(r'timeout=\d+', code):
        raise Error("Use config.TIMEOUT instead of hardcoded value")
```

**3. AI prompt engineering:**

```
- Specify: "Generate test for staging with 30s timeout and retry logic"
- Review: Always adjust AI code for real-world conditions
```

**4. Test harness defaults:**

```python
# Override requests to use realistic defaults
class StagingClient(requests.Session):
    def request(self, *args, **kwargs):
        kwargs.setdefault('timeout', 30)  # Realistic default
        kwargs.setdefault('retries', 3)
        return super().request(*args, **kwargs)
```

---

#### Issue 2: Ignored Edge Case - Cache Invalidation (Bug #4)

**What happened:**

- AI generated cache invalidation code for single-region deployment
- Code only invalidates local Redis, doesn't broadcast to other regions
- Works perfectly in dev/staging (single region) but fails in prod (multi-region)

**AI-generated code (problematic):**

```python
# Generated by AI - assumes single Redis instance
def revoke_api_key(key_id):
    # Update database
    db.execute("UPDATE api_keys SET status='revoked' WHERE id=?", key_id)

    # Invalidate cache
    redis.delete(f"api_key:{key_id}:status")  # ❌ Only local Redis

    return {"success": True}
```

**What AI missed:**

- Production has 6 regional Redis clusters (US, EU, APAC)
- Invalidation must broadcast to all regions (Pub/Sub)
- APAC clusters never receive invalidation event

**Why AI caused this:**

- AI generated code based on simple examples (single-server architecture)
- Didn't ask clarifying questions ("How many Redis instances?")
- Assumed ideal case (no distributed systems complexity)

**How to detect:**

- ⚠️ Regional testing reveals discrepancy (works in US, fails in APAC)
- ⚠️ Customer complaints from specific countries
- ⚠️ Cache hit logs show stale data in certain regions

**How to prevent:**

**1. Architecture docs for AI context:**

```python
# System Architecture (provide to AI as context)
- Multi-region deployment: US, EU, APAC
- Redis: 6 clusters (2 per region, HA)
- Cache invalidation: Must use Pub/Sub to all clusters
```

**2. Code review focus on distributed systems:**

```
- [ ] Cache operations broadcast to all regions?
- [ ] Tested in multi-region staging environment?
- [ ] Eventual consistency handled (graceful degradation)?
```

**3. Integration tests per region:**

```python
@pytest.mark.parametrize("region", ["us-east-1", "eu-west-1", "ap-southeast-1"])
def test_cache_invalidation_global(region):
    # Create key
    key = create_key()

    # Revoke in US region
    revoke_key(key.id, region="us-east-1")

    # Verify invalidated in THIS region
    cache = get_redis_client(region)
    assert cache.get(f"api_key:{key.id}:status") is None
```

**4. QA checklist for AI-generated code:**

```
- [ ] Distributed systems edge cases considered?
- [ ] Multi-region behavior tested?
- [ ] Network partition scenarios handled?
```

### Prevention Strategy (QA Lead Responsibility)

**1. AI Code Review Process:**

- Tag AI-generated code in PRs: `[AI-ASSISTED]`
- Require human review + testing before merge
- QA validates in staging (not just unit tests)

**2. Test Coverage Requirements:**

AI-generated code must have:

- Unit tests (AI can generate these)
- Integration tests (human writes, understands system)
- Edge case tests (human designs based on architecture)

**3. Flakiness Monitoring:**

- Track which tests were AI-generated
- Alert if AI tests have >2x flakiness rate vs human tests
- Review and rewrite problematic AI tests

**4. Quarterly AI Code Audit:**

- Review all AI-generated code from last quarter
- Identify patterns (timeout issues, missing edge cases)
- Update AI prompts / guidelines
- Train team on common AI pitfalls

---
